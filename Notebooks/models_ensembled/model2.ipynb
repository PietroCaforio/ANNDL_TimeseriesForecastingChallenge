{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7176744,"sourceType":"datasetVersion","datasetId":4147411}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Fix randomness and hide warnings\nseed = 42\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\nimport numpy as np\nnp.random.seed(seed)\n\nimport logging\n\nimport random\nrandom.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:22.105871Z","iopub.execute_input":"2023-12-17T11:39:22.106132Z","iopub.status.idle":"2023-12-17T11:39:22.117947Z","shell.execute_reply.started":"2023-12-17T11:39:22.106108Z","shell.execute_reply":"2023-12-17T11:39:22.117024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:25.027756Z","iopub.execute_input":"2023-12-17T11:39:25.031914Z","iopub.status.idle":"2023-12-17T11:39:36.746963Z","shell.execute_reply.started":"2023-12-17T11:39:25.031855Z","shell.execute_reply":"2023-12-17T11:39:36.745910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16)\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import RobustScaler","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:36.748588Z","iopub.execute_input":"2023-12-17T11:39:36.749299Z","iopub.status.idle":"2023-12-17T11:39:37.438952Z","shell.execute_reply.started":"2023-12-17T11:39:36.749261Z","shell.execute_reply":"2023-12-17T11:39:37.438133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"data = np.load('/kaggle/input/datasetchallengeann2/training_dataset/training_data.npy')\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:42.191924Z","iopub.execute_input":"2023-12-17T11:39:42.192783Z","iopub.status.idle":"2023-12-17T11:39:49.307033Z","shell.execute_reply.started":"2023-12-17T11:39:42.192749Z","shell.execute_reply":"2023-12-17T11:39:49.306073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories = np.load('/kaggle/input/datasetchallengeann2/training_dataset/categories.npy')\nvalid_indices = np.load('/kaggle/input/datasetchallengeann2/training_dataset/valid_periods.npy')\nprint(categories.shape)\nprint(valid_indices.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:49.308580Z","iopub.execute_input":"2023-12-17T11:39:49.308880Z","iopub.status.idle":"2023-12-17T11:39:49.327100Z","shell.execute_reply.started":"2023-12-17T11:39:49.308855Z","shell.execute_reply":"2023-12-17T11:39:49.326249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate mean window on the sequences","metadata":{}},{"cell_type":"code","source":"def find_window(time_series,threshold):\n    '''\n        This function calculates the window on a sequence \n        based on a certain threshold of the autocorrelation function.\n        parameters:\n        -time_series: the sequence on which to calculate the window\n        -threshold: autocorrelation value on which the window is selected\n    '''\n    autocorr = np.correlate(time_series,time_series, mode = 'full') #calculate autocorrelation function\n    \n    autocorr /= autocorr[int(len(autocorr)/2)] \n    autocorr = autocorr[int(len(autocorr)/2):]\n    \n    window = np.argmax(autocorr <= threshold) #select window size based on threshold\n    return window","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:52.010662Z","iopub.execute_input":"2023-12-17T11:39:52.011564Z","iopub.status.idle":"2023-12-17T11:39:52.016520Z","shell.execute_reply.started":"2023-12-17T11:39:52.011522Z","shell.execute_reply":"2023-12-17T11:39:52.015665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Put the windows of the entire dataset in a numpy array and calculate the mean value\nwindows = []\nfor time_series in X_train_raw:\n    windows.append(find_window(time_series,threshold=0.2))\n\nprint(\"Mean window\",np.mean(np.array(windows))) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_window = 123 #hardcoded for practical purposes","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:55.526129Z","iopub.execute_input":"2023-12-17T11:39:55.526526Z","iopub.status.idle":"2023-12-17T11:39:55.530928Z","shell.execute_reply.started":"2023-12-17T11:39:55.526494Z","shell.execute_reply":"2023-12-17T11:39:55.529982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_random_timeseries(data, labels, types=('A', 'B', 'C', 'D', 'E')):\n    \"\"\"\n    Plot one random time series for each type specified in 'types'.\n\n    Parameters:\n    - data: numpy array of shape (num_time_series, time_series_length)\n    - labels: numpy array of shape (num_time_series,) representing the type for each time series\n    - types: tuple specifying the types to plot (default is ('A', 'B', 'C', 'D', 'E'))\n    \"\"\"\n    # Create a figure with subplots for each type\n    fig, axs = plt.subplots(len(types), 1, figsize=(10, 2 * len(types)), sharex=True, sharey=True)\n\n    # Loop through each type and plot a random time series of that type\n    for i, t in enumerate(types):\n        # Get indices of time series with the specified type\n        type_indices = np.where(labels == t)[0]\n        \n        # Select a random index for the given type\n        random_index = np.random.choice(type_indices)\n        \n        # Extract the time series data\n        time_series = data[random_index, valid_indices[random_index, 0]:]\n        \n        # Plot the time series\n        axs[i].plot(time_series)\n        axs[i].set_title(f'Type {t}')\n\n    # Adjust layout and show the plot\n    plt.tight_layout()\n    plt.show()\n\n# Example usage:\n# Assuming 'data' is your time series array and 'labels' is your type array\n# plot_random_timeseries(data, labels)\nplot_random_timeseries(data, categories)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:39:56.736157Z","iopub.execute_input":"2023-12-17T11:39:56.736520Z","iopub.status.idle":"2023-12-17T11:39:57.839184Z","shell.execute_reply.started":"2023-12-17T11:39:56.736490Z","shell.execute_reply":"2023-12-17T11:39:57.838157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Randomly select 10 time series indices\nrandom_indices = np.random.choice(data.shape[0], 10, replace=False)\n\n# Plot each selected time series in a separate subplot\nnum_plots = len(random_indices)\nfig, axs = plt.subplots(num_plots, 1, figsize=(10, 2 * num_plots))\n\nfor i, index in enumerate(random_indices):\n    axs[i].plot(data[index, valid_indices[index, 0]:])\n    axs[i].set_title(f'Time Series {index}')\n    axs[i].set_xlabel('Time')\n    axs[i].set_ylabel('Value')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:40:03.387921Z","iopub.execute_input":"2023-12-17T11:40:03.388523Z","iopub.status.idle":"2023-12-17T11:40:05.216478Z","shell.execute_reply.started":"2023-12-17T11:40:03.388491Z","shell.execute_reply":"2023-12-17T11:40:05.215497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train test split and build the sequences","metadata":{}},{"cell_type":"code","source":"def build_sequences(df, window=100, stride=20, telescope=100):\n    global count_inspect\n    # Sanity check to avoid runtime errors\n    assert window % stride == 0\n    dataset = []\n    labels = []\n    df = np.expand_dims(df, axis = -1)\n    temp_df = np.copy(df)\n    padding_check = len(df)%(window+telescope) #compute a boolean that says if the padding needs to be done\n\n    if(padding_check != 0):\n        # Compute padding length\n        padding_len = window+telescope - len(df)%(window+telescope) # compute padding length\n        padding = np.array([]) #padding starts empty\n        padding = np.expand_dims(padding, axis=-1)\n        if(padding_len//len(temp_df)>0): #If the whole timeseries can be copied entirely an integer number of times (at least 1)...\n            for i in range(0,padding_len//len(temp_df)):\n                noise = np.random.normal(0,1, (len(temp_df),1)) #generate some random noise\n                sum_ = np.sum([temp_df,noise], axis = 0) #copy the whole timeseries and add the noise to it\n                padding = np.concatenate((sum_,padding)) #concatenate the new padding piece with the padding up to now\n\n        if(padding_len % len(temp_df) !=0): #if still there is some gap to fill, but the timeseries doesn't fit entirely\n            noise = np.random.normal(0,1, (padding_len % len(temp_df),1)) #generate some random noise\n            last_copy = temp_df[-(padding_len % len(temp_df)):] #copy a piece of the timeseries large enough to fill the gap\n            last_pad = np.sum([last_copy, noise], axis = 0) #add the noise\n            padding = np.concatenate((last_pad,padding)) #concatenate the new padding piece with the padding up to now\n        if(np.max(padding)-np.min(padding) != 0):  #check to avoid division by zero\n            padding = (padding - np.min(padding))/np.ptp(padding) #normalize the padding\n\n        temp_df = np.concatenate((padding,df)) #concatenate the padding to the timeseries\n        assert len(temp_df) % (window+telescope) == 0 #verify we have correctly padded\n\n    for idx in np.arange(0,len(temp_df)-window-telescope +1 ,stride):\n        dataset.append(temp_df[idx:idx+window]) #take the first piece of length <window> as data\n        labels.append(temp_df[idx+window:idx+window+telescope]) #take the following piece as telescope\n\n    dataset = np.array(dataset)\n    labels = np.array(labels)\n    return dataset, labels #return data","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:40:05.582171Z","iopub.execute_input":"2023-12-17T11:40:05.582866Z","iopub.status.idle":"2023-12-17T11:40:05.594894Z","shell.execute_reply.started":"2023-12-17T11:40:05.582834Z","shell.execute_reply":"2023-12-17T11:40:05.594040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install joblib","metadata":{"execution":{"iopub.status.busy":"2023-12-16T18:58:44.310165Z","iopub.execute_input":"2023-12-16T18:58:44.310561Z","iopub.status.idle":"2023-12-16T18:58:56.947820Z","shell.execute_reply.started":"2023-12-16T18:58:44.310532Z","shell.execute_reply":"2023-12-16T18:58:56.946603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport joblib\nX_train_raw, X_validation_raw, valid_indices_train, valid_indices_validation = train_test_split(data, valid_indices, test_size = 0.2)\nprint(X_train_raw.shape, X_validation_raw.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:40:13.371984Z","iopub.execute_input":"2023-12-17T11:40:13.372715Z","iopub.status.idle":"2023-12-17T11:40:13.704763Z","shell.execute_reply.started":"2023-12-17T11:40:13.372685Z","shell.execute_reply":"2023-12-17T11:40:13.703859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(valid_indices_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:40:16.950849Z","iopub.execute_input":"2023-12-17T11:40:16.951588Z","iopub.status.idle":"2023-12-17T11:40:16.956179Z","shell.execute_reply.started":"2023-12-17T11:40:16.951558Z","shell.execute_reply":"2023-12-17T11:40:16.955254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lengths = []\nfor i in range(len(valid_indices)):\n    lengths.append(valid_indices[i, 1]-valid_indices[i, 0])\nlengths = np.array(lengths)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:40:17.699620Z","iopub.execute_input":"2023-12-17T11:40:17.700327Z","iopub.status.idle":"2023-12-17T11:40:17.747741Z","shell.execute_reply.started":"2023-12-17T11:40:17.700297Z","shell.execute_reply":"2023-12-17T11:40:17.746806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoregressive_telescope=3 ","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:40:19.765920Z","iopub.execute_input":"2023-12-17T11:40:19.766650Z","iopub.status.idle":"2023-12-17T11:40:19.770693Z","shell.execute_reply.started":"2023-12-17T11:40:19.766620Z","shell.execute_reply":"2023-12-17T11:40:19.769732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = []\ny_train = []\nfor i, time_series in enumerate(X_train_raw):\n    dataset_tmp, labels_tmp = build_sequences(time_series[valid_indices_train[i, 0]:], window = mean_window, stride = 3, telescope = autoregressive_telescope)\n    X_train.extend(dataset_tmp)\n    y_train.extend(labels_tmp)\nX_train = np.array(X_train)\ny_train = np.array(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:41:00.194722Z","iopub.execute_input":"2023-12-17T11:41:00.195346Z","iopub.status.idle":"2023-12-17T11:41:14.589964Z","shell.execute_reply.started":"2023-12-17T11:41:00.195316Z","shell.execute_reply":"2023-12-17T11:41:14.588890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val = []\ny_val = []\nfor i, time_series in enumerate(X_validation_raw):\n    dataset_tmp, labels_tmp = build_sequences(time_series[valid_indices_validation[i, 0]:], window = mean_window, stride = 3, telescope = autoregressive_telescope)\n    X_val.extend(dataset_tmp)\n    y_val.extend(labels_tmp)\nX_val = np.array(X_val)\ny_val = np.array(y_val)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:41:19.727314Z","iopub.execute_input":"2023-12-17T11:41:19.727697Z","iopub.status.idle":"2023-12-17T11:41:23.078429Z","shell.execute_reply.started":"2023-12-17T11:41:19.727668Z","shell.execute_reply":"2023-12-17T11:41:23.077436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:41:26.228354Z","iopub.execute_input":"2023-12-17T11:41:26.229078Z","iopub.status.idle":"2023-12-17T11:41:26.233675Z","shell.execute_reply.started":"2023-12-17T11:41:26.229045Z","shell.execute_reply":"2023-12-17T11:41:26.232808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 1024\nepochs = 300","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:48:21.004188Z","iopub.execute_input":"2023-12-17T11:48:21.004915Z","iopub.status.idle":"2023-12-17T11:48:21.009396Z","shell.execute_reply.started":"2023-12-17T11:48:21.004883Z","shell.execute_reply":"2023-12-17T11:48:21.008409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inspect_multivariate(X, y, telescope, idx=None):\n    random_indices = np.random.choice(X.shape[0], 10, replace=False)\n\n    # Plot each selected time series in a separate subplot\n    num_plots = len(random_indices)\n    fig, axs = plt.subplots(num_plots, 1, figsize=(10, 2 * num_plots))\n\n    for i, index in enumerate(random_indices):\n        axs[i].plot(np.arange(len(X[0,:])), X[index,:])\n        axs[i].scatter(np.arange(len(X[0,:]), len(X[0,:])+telescope), y[index,:], color='orange')\n        axs[i].set_title(f'Time Series {index}')\n        axs[i].set_xlabel('Time')\n        axs[i].set_ylabel('Value')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:41:44.097710Z","iopub.execute_input":"2023-12-17T11:41:44.098354Z","iopub.status.idle":"2023-12-17T11:41:44.105956Z","shell.execute_reply.started":"2023-12-17T11:41:44.098322Z","shell.execute_reply":"2023-12-17T11:41:44.104975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspect_multivariate(X_train, y_train, autoregressive_telescope)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:41:48.619982Z","iopub.execute_input":"2023-12-17T11:41:48.620318Z","iopub.status.idle":"2023-12-17T11:41:50.496436Z","shell.execute_reply.started":"2023-12-17T11:41:48.620293Z","shell.execute_reply":"2023-12-17T11:41:50.495533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build and train the model","metadata":{}},{"cell_type":"code","source":"def build_resnet(input_shape, n_feature_maps = 64):\n    '''\n        This function returns the input and output layer of a 1D based resnet architecture\n    '''\n    input_layer = tfkl.Input(input_shape)\n\n    # BLOCK 1\n\n    conv_x = tfkl.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n    conv_x = tfkl.BatchNormalization()(conv_x)\n    conv_x = tfkl.Activation('relu')(conv_x)\n\n    conv_y = tfkl.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n    conv_y = tfkl.BatchNormalization()(conv_y)\n    conv_y = tfkl.Activation('relu')(conv_y)\n\n    conv_z = tfkl.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n    conv_z = tfkl.BatchNormalization()(conv_z)\n\n    # expand channels for the sum\n    shortcut_y = tfkl.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n    shortcut_y = tfkl.BatchNormalization()(shortcut_y)\n\n    output_block_1 = tfkl.add([shortcut_y, conv_z])\n    output_block_1 = tfkl.Activation('relu')(output_block_1)\n\n    # BLOCK 2\n\n    conv_x = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n    conv_x = tfkl.BatchNormalization()(conv_x)\n    conv_x = tfkl.Activation('relu')(conv_x)\n\n    conv_y = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n    conv_y = tfkl.BatchNormalization()(conv_y)\n    conv_y = tfkl.Activation('relu')(conv_y)\n\n    conv_z = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n    conv_z = tfkl.BatchNormalization()(conv_z)\n\n    # expand channels for the sum\n    shortcut_y = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n    shortcut_y = tfkl.BatchNormalization()(shortcut_y)\n\n    output_block_2 = tfkl.add([shortcut_y, conv_z])\n    output_block_2 = tfkl.Activation('relu')(output_block_2)\n\n    # BLOCK 3\n\n    conv_x = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n    conv_x = tfkl.BatchNormalization()(conv_x)\n    conv_x = tfkl.Activation('relu')(conv_x)\n\n    conv_y = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n    conv_y = tfkl.BatchNormalization()(conv_y)\n    conv_y = tfkl.Activation('relu')(conv_y)\n\n    conv_z = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n    conv_z = tfkl.BatchNormalization()(conv_z)\n\n    # no need to expand channels because they are equal\n    shortcut_y = tfkl.BatchNormalization()(output_block_2)\n\n    output_block_3 = tfkl.add([shortcut_y, conv_z])\n    output_block_3 = tfkl.Activation('relu')(output_block_3)\n\n    # FINAL\n\n    out = tfkl.GlobalAveragePooling1D()(output_block_3)\n\n    print ('        -- model was built.')\n    return input_layer, out","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:48:26.939078Z","iopub.execute_input":"2023-12-17T11:48:26.939813Z","iopub.status.idle":"2023-12-17T11:48:26.955449Z","shell.execute_reply.started":"2023-12-17T11:48:26.939780Z","shell.execute_reply":"2023-12-17T11:48:26.954139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_CONV_LSTM_model(input_shape, output_shape, n_resnet_features = 32,n_features = 64):#merge convolution and lstm, to extract features and correlate in the time domain\n    # Ensure the input time steps are at least as many as the output time steps\n    assert input_shape[0] >= output_shape[0], \"For this exercise we want input time steps to be >= of output time steps\"\n    \n    input_layer, embedding = build_resnet(input_shape,n_resnet_features)\n    \n    #embedding = tfkl.Reshape((1,embedding.get_output_at(0).get_shape().as_list()[1]))(embedding)\n    embedding = tf.expand_dims(embedding,-1)\n    # Add a Bidirectional LSTM layer with 64 units\n    x = tfkl.Bidirectional(tfkl.LSTM(n_features, return_sequences=True, name='lstm'), name='bidirectional_lstm')(embedding)\n\n    # Add a 1D Convolution layer with 128 filters and a kernel size of 3\n    x = tfkl.Conv1D(n_features*2, 3, padding='same', activation='relu', name='conv')(x)\n    # Add a 1D Convolution layer with 128 filters and a kernel size of 3\n    x = tfkl.Conv1D(n_features*2, 3, padding='same', activation='relu', name='conv1')(x)\n\n    # Add a final Convolution layer to match the desired output shape\n    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same', name='output_layer')(x)\n    \n    # Calculate the size to crop from the output to match the output shape\n    crop_size = output_layer.shape[1] - output_shape[0]\n\n    # Crop the output to the desired length\n    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)#CROPPO AL POSTO DI USARE AD ESEMPIO MAX POOLING IN QUANTO AVERE DIMENSIONI STRANE SAREBBE PROBLEMATICO CON MAX POOLING\n\n    # Construct the model by connecting input and output layers\n    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n\n    # Compile the model with Mean Squared Error loss and Adam optimizer\n    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:48:27.585954Z","iopub.execute_input":"2023-12-17T11:48:27.586633Z","iopub.status.idle":"2023-12-17T11:48:27.595984Z","shell.execute_reply.started":"2023-12-17T11:48:27.586599Z","shell.execute_reply":"2023-12-17T11:48:27.595032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_CONV_LSTM_model(input_shape, output_shape, n_features = 128)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:48:31.850974Z","iopub.execute_input":"2023-12-17T11:48:31.851605Z","iopub.status.idle":"2023-12-17T11:48:36.390411Z","shell.execute_reply.started":"2023-12-17T11:48:31.851572Z","shell.execute_reply":"2023-12-17T11:48:36.389457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data=(X_val, y_val),\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=12, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n    ]\n).history","metadata":{"execution":{"iopub.status.busy":"2023-12-17T11:48:36.391971Z","iopub.execute_input":"2023-12-17T11:48:36.392242Z","iopub.status.idle":"2023-12-17T11:49:59.907129Z","shell.execute_reply.started":"2023-12-17T11:48:36.392218Z","shell.execute_reply":"2023-12-17T11:49:59.905612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('forecasting_all_resnet_aug.h5')","metadata":{},"execution_count":null,"outputs":[]}]}